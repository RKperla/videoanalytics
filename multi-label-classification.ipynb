{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "from keras_frcnn.mylayer import mylayer\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "K.set_learning_phase(1) #set learning phase\n",
    "\n",
    "# sys.setrecursionlimit(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network ='vgg'\n",
    "\n",
    "if network == 'vgg':\n",
    "\tfrom keras_frcnn import vgg as nn\n",
    "elif network == 'resnet50':\n",
    "\tfrom keras_frcnn import resnet as nn\n",
    "else:\n",
    "\tprint('Not a valid model')\n",
    "\traise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained model\n"
     ]
    }
   ],
   "source": [
    "from keras_frcnn.RoiPoolingConv import RoiPoolingConv\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "\tinput_shape_img = (3, None, None)\n",
    "else:\n",
    "\tinput_shape_img = (224,224, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "\n",
    "out = (Flatten(name='flatten'))(shared_layers)\n",
    "out = (Dense(4096, activation='relu', name='fc5'))(out)\n",
    "out = (Dropout(0.5))(out)\n",
    "out = (Dense(2048, activation='relu', name='fc6'))(out)\n",
    "# out = (Dropout(0.5))(out)\n",
    "# out = (Dense(512, activation='relu', name='fc7'))(out)\n",
    "out_class = (Dense(1000, activation='sigmoid'))(out)\n",
    "\n",
    "model_classifier = Model(img_input, out_class)\n",
    "\n",
    "\n",
    "try:\n",
    "\tprint('loading weights from pretrained model')\n",
    "\tmodel_classifier.load_weights('vgg16_weights_tf_dim_ordering_tf_kernels.h5', by_name=True)\n",
    "except:\n",
    "\tprint('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
    "\t\thttps://github.com/fchollet/keras/tree/master/keras/applications')\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss = 'binary_crossentropy', metrics= ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(x):\n",
    "    name = str.split(x,'/')\n",
    "    return '/Users/1024329/Downloads/DeepFashion/img_n/' + name[1] + '/' + name[2]\n",
    "    \n",
    "df3 = pd.read_pickle('upper_body_atts.pkl')\n",
    "df3['img_name'] = df3['img_name'].apply(lambda x:name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>atts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Lon...</td>\n",
       "      <td>[[96, 2, 60, 60], [165, 2, 60, 60], [300, 153,...</td>\n",
       "      <td>[99, 495, 571, 881, 940]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Hoo...</td>\n",
       "      <td>[[105, 5, 60, 60], [255, 3, 60, 60], [26, 195,...</td>\n",
       "      <td>[470, 953]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Rea...</td>\n",
       "      <td>[[126, 36, 60, 60], [244, 42, 60, 60], [67, 3,...</td>\n",
       "      <td>[745, 883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Flo...</td>\n",
       "      <td>[[67, 8, 60, 60], [217, 2, 60, 60], [24, 167, ...</td>\n",
       "      <td>[142, 306, 365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Bea...</td>\n",
       "      <td>[[51, 68, 60, 60], [280, 80, 60, 60], [25, 13,...</td>\n",
       "      <td>[50, 121, 122, 226, 681]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_name  \\\n",
       "0  /Users/1024329/Downloads/DeepFashion/img_n/Lon...   \n",
       "1  /Users/1024329/Downloads/DeepFashion/img_n/Hoo...   \n",
       "2  /Users/1024329/Downloads/DeepFashion/img_n/Rea...   \n",
       "3  /Users/1024329/Downloads/DeepFashion/img_n/Flo...   \n",
       "4  /Users/1024329/Downloads/DeepFashion/img_n/Bea...   \n",
       "\n",
       "                                           landmarks                      atts  \n",
       "0  [[96, 2, 60, 60], [165, 2, 60, 60], [300, 153,...  [99, 495, 571, 881, 940]  \n",
       "1  [[105, 5, 60, 60], [255, 3, 60, 60], [26, 195,...                [470, 953]  \n",
       "2  [[126, 36, 60, 60], [244, 42, 60, 60], [67, 3,...                [745, 883]  \n",
       "3  [[67, 8, 60, 60], [217, 2, 60, 60], [24, 167, ...           [142, 306, 365]  \n",
       "4  [[51, 68, 60, 60], [280, 80, 60, 60], [25, 13,...  [50, 121, 122, 226, 681]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3[['img_name','landmarks','atts']]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the samples without any attribute present in it\n",
    "sample = df3['atts'].apply(lambda x: x.shape[0]>0)\n",
    "df4 =df3[sample]\n",
    "df4.shape, df3.shape\n",
    "# checking the maximum value of the attribute label\n",
    "df4['atts'].apply(lambda x: np.max(x)).max()\n",
    "\n",
    "# reset index of df4\n",
    "df4 = df4.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132290, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "vals = np.array([])\n",
    "for i in range(df4.shape[0]):\n",
    "    if i%10000 ==0: print i\n",
    "    vals = np.concatenate([vals,df4.loc[i,'atts']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "his = plt.hist(vals,bins =1000)\n",
    "select =  np.nonzero(his[0] > 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(x,select):\n",
    "    s = [i for i in x if i in (select[0])]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['select46_atts'] = df4['atts'].apply(lambda x: selecting(x,select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>atts</th>\n",
       "      <th>select46_atts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Lon...</td>\n",
       "      <td>[[96, 2, 60, 60], [165, 2, 60, 60], [300, 153,...</td>\n",
       "      <td>[99, 495, 571, 881, 940]</td>\n",
       "      <td>[495, 571]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Hoo...</td>\n",
       "      <td>[[105, 5, 60, 60], [255, 3, 60, 60], [26, 195,...</td>\n",
       "      <td>[470, 953]</td>\n",
       "      <td>[470]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Rea...</td>\n",
       "      <td>[[126, 36, 60, 60], [244, 42, 60, 60], [67, 3,...</td>\n",
       "      <td>[745, 883]</td>\n",
       "      <td>[745, 883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Flo...</td>\n",
       "      <td>[[67, 8, 60, 60], [217, 2, 60, 60], [24, 167, ...</td>\n",
       "      <td>[142, 306, 365]</td>\n",
       "      <td>[142, 365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Bea...</td>\n",
       "      <td>[[51, 68, 60, 60], [280, 80, 60, 60], [25, 13,...</td>\n",
       "      <td>[50, 121, 122, 226, 681]</td>\n",
       "      <td>[226]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_name  \\\n",
       "0  /Users/1024329/Downloads/DeepFashion/img_n/Lon...   \n",
       "1  /Users/1024329/Downloads/DeepFashion/img_n/Hoo...   \n",
       "2  /Users/1024329/Downloads/DeepFashion/img_n/Rea...   \n",
       "3  /Users/1024329/Downloads/DeepFashion/img_n/Flo...   \n",
       "4  /Users/1024329/Downloads/DeepFashion/img_n/Bea...   \n",
       "\n",
       "                                           landmarks  \\\n",
       "0  [[96, 2, 60, 60], [165, 2, 60, 60], [300, 153,...   \n",
       "1  [[105, 5, 60, 60], [255, 3, 60, 60], [26, 195,...   \n",
       "2  [[126, 36, 60, 60], [244, 42, 60, 60], [67, 3,...   \n",
       "3  [[67, 8, 60, 60], [217, 2, 60, 60], [24, 167, ...   \n",
       "4  [[51, 68, 60, 60], [280, 80, 60, 60], [25, 13,...   \n",
       "\n",
       "                       atts select46_atts  \n",
       "0  [99, 495, 571, 881, 940]    [495, 571]  \n",
       "1                [470, 953]         [470]  \n",
       "2                [745, 883]    [745, 883]  \n",
       "3           [142, 306, 365]    [142, 365]  \n",
       "4  [50, 121, 122, 226, 681]         [226]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106702, 4) (132290, 4)\n",
      "993\n"
     ]
    }
   ],
   "source": [
    "# removing the samples without any attribute present in it\n",
    "sample = df4['select46_atts'].apply(lambda x: np.array(x).shape[0]>0)\n",
    "df5 =df4[sample]\n",
    "print df5.shape, df4.shape\n",
    "# checking the maximum value of the attribute label\n",
    "print df5['select46_atts'].apply(lambda x: np.max(x)).max()\n",
    "\n",
    "# reset index of df4\n",
    "df5 = df5.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['select46_atts'] = df5['select46_atts'].apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  93, 112, 142, 162, 181, 196, 207, 212, 226, 227, 254, 310,\n",
       "       335, 365, 380, 441, 470, 495, 513, 546, 568, 571, 574, 577, 601,\n",
       "       640, 695, 705, 720, 730, 745, 751, 754, 760, 818, 822, 836, 837,\n",
       "       883, 884, 892, 935, 956, 983, 993])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106702, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(num):\n",
    "    img = []\n",
    "    attribs = []\n",
    "    b_size = 32\n",
    "    for i in range(b_size*num, b_size*(num+1)):\n",
    "        #print i\n",
    "        a = cv2.imread(df5.loc[i,'img_name'])\n",
    "        a = cv2.resize(a,(224,224))\n",
    "        a = np.array(a).reshape(224,224,3)\n",
    "        img.append(a/255)\n",
    "        atts = df5.loc[i,'select46_atts']\n",
    "        labels = np.array([0 if item not in atts else 1 for item in select[0]]).reshape(46,)\n",
    "        attribs.append(labels)\n",
    "    return np.array(img), np.array(attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_df4(num):\n",
    "    img = []\n",
    "    attribs = []\n",
    "    b_size = 32\n",
    "    for i in range(b_size*num, b_size*(num+1)):\n",
    "        #print i\n",
    "        a = cv2.imread(df4.loc[i,'img_name'])\n",
    "        a = cv2.resize(a,(224,224))\n",
    "        a = np.array(a).reshape(224,224,3)\n",
    "        img.append(a/255)\n",
    "        atts = df4.loc[i,'atts']\n",
    "        #labels = np.array([0 if item not in atts else 1 for item in select[0]]).reshape(1000,)\n",
    "        labels = np.array([0 if k not in atts else 1 for k in range(1000)]).reshape(1000,)\n",
    "        attribs.append(labels)\n",
    "    return np.array(img), np.array(attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = get_batch_df4(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = get_batch(2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  93, 112, 142, 162, 181, 196, 207, 212, 226, 227, 254, 310,\n",
       "       335, 365, 380, 441, 470, 495, 513, 546, 568, 571, 574, 577, 601,\n",
       "       640, 695, 705, 720, 730, 745, 751, 754, 760, 818, 822, 836, 837,\n",
       "       883, 884, 892, 935, 956, 983, 993])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify\n",
    "#df5 = df5.head(35000)\n",
    "df5 = df4.head(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.sample(frac = 1).reset_index(drop =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35000, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/1024329/Downloads/DeepFashion/img_n/Hooded_Cotton_Canvas_Anorak/img_00000135.jpg'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.loc[0,'img_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0\n",
      "epoch and batch: 0 0\n",
      "WARNING:tensorflow:From /anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      " - 34s - loss: 0.7723 - acc: 0.4897\n",
      "epoch and batch: 0 1\n",
      "Epoch 1/1\n",
      " - 28s - loss: 0.7450 - acc: 0.5089\n",
      "epoch and batch: 0 2\n",
      "Epoch 1/1\n",
      " - 23s - loss: 0.7291 - acc: 0.5247\n",
      "epoch and batch: 0 3\n",
      "Epoch 1/1\n",
      " - 24s - loss: 0.7103 - acc: 0.5426\n",
      "epoch and batch: 0 4\n",
      "Epoch 1/1\n",
      " - 23s - loss: 0.6966 - acc: 0.5611\n",
      "epoch and batch: 0 5\n",
      "Epoch 1/1\n",
      " - 23s - loss: 0.6837 - acc: 0.5784\n",
      "epoch and batch: 0 6\n",
      "Epoch 1/1\n",
      " - 23s - loss: 0.6714 - acc: 0.5913\n",
      "epoch and batch: 0 7\n",
      "Epoch 1/1\n",
      " - 24s - loss: 0.6581 - acc: 0.6119\n",
      "epoch and batch: 0 8\n",
      "Epoch 1/1\n",
      " - 24s - loss: 0.6485 - acc: 0.6255\n",
      "epoch and batch: 0 9\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0caf5b22a7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'epoch and batch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_df4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#acc = acc + np.array(history.history['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "# train_acc = []\n",
    "for j in range(50):\n",
    "    df5 = df5.sample(frac = 1).reset_index(drop =True)\n",
    "    print 'epoch number:', j\n",
    "    los = 0\n",
    "    acc = 0\n",
    "    for i in range(1000):\n",
    "        print 'epoch and batch:', j, i \n",
    "        X,Y = get_batch_df4(i)\n",
    "        history = model_classifier.fit(x=X, y= Y, verbose = 2)\n",
    "        los = los + np.array(history.history['loss'])\n",
    "        #acc = acc + np.array(history.history['acc'])\n",
    "        #print los\n",
    "    train_loss.append(los)\n",
    "    model_classifier.save_weights('atts_vgg_model.h5')\n",
    "    #train_acc.append(acc)\n",
    "    #print train_loss, train_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier.load_weights('atts_vgg_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa,ya = get_batch(3)\n",
    "out_att = model_classifier.predict(xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([11, 24, 36]),) (array([ 3, 36]),)\n"
     ]
    }
   ],
   "source": [
    "print np.nonzero(ya[3,:]>0), np.nonzero(out_att[3,:] > 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>landmarks</th>\n",
       "      <th>atts</th>\n",
       "      <th>select46_atts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Lon...</td>\n",
       "      <td>[[96, 2, 60, 60], [165, 2, 60, 60], [300, 153,...</td>\n",
       "      <td>[99, 495, 571, 881, 940]</td>\n",
       "      <td>[495, 571]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Hoo...</td>\n",
       "      <td>[[105, 5, 60, 60], [255, 3, 60, 60], [26, 195,...</td>\n",
       "      <td>[470, 953]</td>\n",
       "      <td>[470]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Rea...</td>\n",
       "      <td>[[126, 36, 60, 60], [244, 42, 60, 60], [67, 3,...</td>\n",
       "      <td>[745, 883]</td>\n",
       "      <td>[745, 883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Flo...</td>\n",
       "      <td>[[67, 8, 60, 60], [217, 2, 60, 60], [24, 167, ...</td>\n",
       "      <td>[142, 306, 365]</td>\n",
       "      <td>[142, 365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/1024329/Downloads/DeepFashion/img_n/Bea...</td>\n",
       "      <td>[[51, 68, 60, 60], [280, 80, 60, 60], [25, 13,...</td>\n",
       "      <td>[50, 121, 122, 226, 681]</td>\n",
       "      <td>[226]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_name  \\\n",
       "0  /Users/1024329/Downloads/DeepFashion/img_n/Lon...   \n",
       "1  /Users/1024329/Downloads/DeepFashion/img_n/Hoo...   \n",
       "2  /Users/1024329/Downloads/DeepFashion/img_n/Rea...   \n",
       "3  /Users/1024329/Downloads/DeepFashion/img_n/Flo...   \n",
       "4  /Users/1024329/Downloads/DeepFashion/img_n/Bea...   \n",
       "\n",
       "                                           landmarks  \\\n",
       "0  [[96, 2, 60, 60], [165, 2, 60, 60], [300, 153,...   \n",
       "1  [[105, 5, 60, 60], [255, 3, 60, 60], [26, 195,...   \n",
       "2  [[126, 36, 60, 60], [244, 42, 60, 60], [67, 3,...   \n",
       "3  [[67, 8, 60, 60], [217, 2, 60, 60], [24, 167, ...   \n",
       "4  [[51, 68, 60, 60], [280, 80, 60, 60], [25, 13,...   \n",
       "\n",
       "                       atts select46_atts  \n",
       "0  [99, 495, 571, 881, 940]    [495, 571]  \n",
       "1                [470, 953]         [470]  \n",
       "2                [745, 883]    [745, 883]  \n",
       "3           [142, 306, 365]    [142, 365]  \n",
       "4  [50, 121, 122, 226, 681]         [226]  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "# model_classifier.fit(x = [X,X2],y= Y, batch_size=1, epochs=2,validation_split=0.2,callbacks = [checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
